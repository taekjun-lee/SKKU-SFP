{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecd3b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, BatchNormalization,\n",
    "    GRU, LSTM, Bidirectional,\n",
    "    Conv1D, MaxPooling1D, GlobalAveragePooling1D,\n",
    "    LayerNormalization, MultiHeadAttention, Add\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6874d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEETJ\\AppData\\Local\\Temp\\ipykernel_6516\\1390813879.py:16: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  data = data.interpolate(method='time')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04',\n",
       "       'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09',\n",
       "       'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14',\n",
       "       'sensor_16', 'sensor_17', 'sensor_19', 'sensor_20', 'sensor_21',\n",
       "       'sensor_22', 'sensor_23', 'sensor_24', 'sensor_25', 'sensor_26',\n",
       "       'sensor_27', 'sensor_28', 'sensor_29', 'sensor_30', 'sensor_31',\n",
       "       'sensor_32', 'sensor_33', 'sensor_34', 'sensor_35', 'sensor_36',\n",
       "       'sensor_37', 'sensor_38', 'sensor_39', 'sensor_40', 'sensor_41',\n",
       "       'sensor_42', 'sensor_43', 'sensor_44', 'sensor_45', 'sensor_46',\n",
       "       'sensor_47', 'sensor_48', 'sensor_49', 'sensor_50', 'sensor_51',\n",
       "       'machine_status_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "data = pd.read_csv(\"../frontend/public/sensor.csv\")\n",
    "\n",
    "all_zero_cols = data.columns[(data == 0).all()].tolist()\n",
    "all_nan_cols = data.columns[data.isna().all()].tolist()\n",
    "threshold = 1.2\n",
    "low_std_cols = data.std(numeric_only=True).loc[lambda x: x < threshold].index.tolist()\n",
    "useless_columns = list(set(all_zero_cols + all_nan_cols + low_std_cols))\n",
    "data = data.drop(columns=useless_columns)\n",
    "\n",
    "status_map = {'NORMAL': 0, 'RECOVERING': 1, 'BROKEN': 2}\n",
    "data['machine_status_encoded'] = data['machine_status'].map(status_map)\n",
    "\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data.set_index('timestamp', inplace=True)\n",
    "data = data.interpolate(method='time')\n",
    "\n",
    "sensor_cols = [col for col in data.columns if col.startswith('sensor')]\n",
    "scaler = StandardScaler()\n",
    "data[sensor_cols] = scaler.fit_transform(data[sensor_cols])\n",
    "\n",
    "data = data.drop(columns=['machine_status'])\n",
    "data = data.sort_index()\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8b2210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220320\n",
      "132192\n",
      "88128\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할 학습(60%), 테스트(40%)\n",
    "total_rows = len(data)\n",
    "split_idx = int(total_rows * 0.6)\n",
    "train_data = data.iloc[:split_idx]\n",
    "test_data = data.iloc[split_idx:]\n",
    "\n",
    "print(total_rows)\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe613a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트렌드 예측 window 생성\n",
    "def create_sensor_trend_windows(data, sensor_cols, window_size=60, step=1):\n",
    "    sensor_data = data[sensor_cols].to_numpy(dtype=np.float32)\n",
    "    num_windows = (len(sensor_data) - window_size) // step\n",
    "\n",
    "    X = np.empty((num_windows, window_size, len(sensor_cols)), dtype=np.float32)\n",
    "    y = np.empty((num_windows, len(sensor_cols)), dtype=np.float32)\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        idx = i * step\n",
    "        X[i] = sensor_data[idx:idx + window_size]\n",
    "        y[i] = sensor_data[idx + window_size]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = create_sensor_trend_windows(train_data, sensor_cols)\n",
    "X_test, y_test = create_sensor_trend_windows(test_data, sensor_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad33426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13213\n",
      "13213\n"
     ]
    }
   ],
   "source": [
    "# 데이터 축소 (10% 샘플링)\n",
    "subset_size = int(len(X_train) * 0.1)\n",
    "X_train_small = X_train[:subset_size]\n",
    "y_train_small = y_train[:subset_size]\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    mse_list = [mean_squared_error(y_true[:, i], y_pred[:, i]) for i in range(len(sensor_cols))]\n",
    "    return pd.DataFrame({'Sensor': sensor_cols, f'{name}_MSE': mse_list})\n",
    "\n",
    "print(len(X_train_small))\n",
    "print(len(y_train_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9b6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% 데이터로 약식 평가\n",
    "# GRU\n",
    "gru_model = Sequential([\n",
    "    GRU(64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(len(sensor_cols))\n",
    "])\n",
    "gru_model.compile(optimizer='adam', loss='mse')\n",
    "gru_model.fit(X_train_small, y_train_small, validation_split=0.2, epochs=5, batch_size=64, verbose=0)\n",
    "gru_y_pred = gru_model.predict(X_test)\n",
    "df_gru = evaluate_model('GRU', y_test, gru_y_pred)\n",
    "\n",
    "# LSTM\n",
    "lstm_model = Sequential([\n",
    "    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(len(sensor_cols))\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_model.fit(X_train_small, y_train_small, validation_split=0.2, epochs=5, batch_size=64, verbose=0)\n",
    "lstm_y_pred = lstm_model.predict(X_test)\n",
    "df_lstm = evaluate_model('LSTM', y_test, lstm_y_pred)\n",
    "\n",
    "# BiLSTM\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(len(sensor_cols))\n",
    "])\n",
    "bilstm_model.compile(optimizer='adam', loss='mse')\n",
    "bilstm_model.fit(X_train_small, y_train_small, validation_split=0.2, epochs=5, batch_size=64, verbose=0)\n",
    "bilstm_y_pred = bilstm_model.predict(X_test)\n",
    "df_bilstm = evaluate_model('BiLSTM', y_test, bilstm_y_pred)\n",
    "\n",
    "# TCN\n",
    "tcn_model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    TCN(nb_filters=32, kernel_size=3, dilations=[1, 2, 4], dropout_rate=0.1),\n",
    "    Dense(len(sensor_cols))\n",
    "])\n",
    "tcn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "tcn_model.fit(X_train_small, y_train_small, validation_split=0.2, epochs=5, batch_size=64, verbose=0, callbacks=[early_stop])\n",
    "tcn_y_pred = tcn_model.predict(X_test)\n",
    "df_tcn = evaluate_model('TCN', y_test, tcn_y_pred)\n",
    "\n",
    "# 1D-CNN\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(len(sensor_cols))\n",
    "])\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "cnn_model.fit(X_train_small, y_train_small, validation_split=0.2, epochs=5, batch_size=64, verbose=0)\n",
    "cnn_y_pred = cnn_model.predict(X_test)\n",
    "df_cnn = evaluate_model('1D_CNN', y_test, cnn_y_pred)\n",
    "\n",
    "# Transformer\n",
    "def transformer_block(inputs, num_heads=2, ff_dim=64, dropout=0.1):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "    attn_output = Dropout(dropout)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "\n",
    "    ff_output = Dense(ff_dim, activation='relu')(out1)\n",
    "    ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "    ff_output = Dropout(dropout)(ff_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ff_output)\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "x = transformer_block(input_layer)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "output_layer = Dense(len(sensor_cols))(x)\n",
    "\n",
    "transformer_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "transformer_model.compile(optimizer='adam', loss='mse')\n",
    "transformer_model.fit(X_train_small, y_train_small, validation_split=0.2, epochs=5, batch_size=64, verbose=0)\n",
    "transformer_y_pred = transformer_model.predict(X_test)\n",
    "df_trans = evaluate_model('Transformer', y_test, transformer_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약식 평가 MSE 결과 저장\n",
    "model_dfs = [df_gru, df_lstm, df_bilstm, df_tcn, df_cnn, df_trans]\n",
    "model_names = ['GRU', 'LSTM', 'BiLSTM', 'TCN', '1D_CNN', 'Transformer']\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "for df, name in zip(model_dfs, model_names):\n",
    "    df_temp = df.copy()\n",
    "    df_temp.columns = ['Sensor', 'MSE']\n",
    "    df_temp['Model'] = name\n",
    "    df_all = pd.concat([df_all, df_temp], axis=0)\n",
    "\n",
    "print(\"모델별 센서 예측 MSE\")\n",
    "display(df_all.pivot(index='Sensor', columns='Model', values='MSE'))\n",
    "\n",
    "# 모델별 평균 성능 출력\n",
    "print(\"모델별 평균 MSE 요약\")\n",
    "avg_mse_summary = df_all.groupby('Model')['MSE'].mean().sort_values()\n",
    "display(avg_mse_summary.round(5).to_frame(name='Average_MSE'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19985063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 모델 전체 데이터로 학습 (GRU, BiLSTM, LSTM)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# GRU\n",
    "gru_trend_model = Sequential([\n",
    "    GRU(128, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(len(sensor_cols))\n",
    "])\n",
    "gru_trend_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "gru_trend_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# BiLSTM\n",
    "bilstm_trend_model = Sequential([\n",
    "    Bidirectional(LSTM(128), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(len(sensor_cols))\n",
    "])\n",
    "bilstm_trend_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "bilstm_trend_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# LSTM\n",
    "lstm_trend_model = Sequential([\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dense(len(sensor_cols))\n",
    "])\n",
    "lstm_trend_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "lstm_trend_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f24ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU, BiLSTM, LSTM 학습 평가\n",
    "models_info = {\n",
    "    \"GRU\": gru_trend_model,\n",
    "    \"LSTM\": lstm_trend_model,\n",
    "    \"BiLSTM\": bilstm_trend_model\n",
    "}\n",
    "\n",
    "all_metrics = {}\n",
    "\n",
    "for model_name, model in models_info.items():\n",
    "    X_init = train_data[sensor_cols].values[-60:].reshape(1, 60, len(sensor_cols))\n",
    "    first_y_pred = model.predict(X_init, verbose=0)\n",
    "    y_pred_rest = model.predict(X_test, verbose=0)\n",
    "    y_pred_full = np.vstack([first_y_pred, y_pred_rest])\n",
    "\n",
    "    metrics = []\n",
    "    for i, sensor in enumerate(sensor_cols):\n",
    "        pred = y_pred_full[:, i]\n",
    "        actual = y_test[:, i]\n",
    "\n",
    "        mse = mean_squared_error(actual, pred[1:])\n",
    "        mae = mean_absolute_error(actual, pred[1:])\n",
    "        r2 = r2_score(actual, pred[1:])\n",
    "\n",
    "        metrics.append({\n",
    "            \"Sensor\": sensor,\n",
    "            \"MSE\": round(mse, 5),\n",
    "            \"MAE\": round(mae, 5),\n",
    "            \"R2\": round(r2, 5)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(metrics)\n",
    "    df[\"Model\"] = model_name\n",
    "    all_metrics[model_name] = df\n",
    "\n",
    "combined_df = pd.concat(all_metrics.values(), ignore_index=True)\n",
    "\n",
    "print(\"센서별 모델 성능 (MSE/MAE/R²)\")\n",
    "display(combined_df.pivot(index='Sensor', columns='Model', values='MSE').round(5))\n",
    "\n",
    "print(\"모델별 평균 성능 요약\")\n",
    "avg_summary_df = combined_df.groupby(\"Model\")[[\"MSE\", \"MAE\", \"R2\"]].mean().round(5)\n",
    "display(avg_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 트렌드 저장 (GRU+KNN 보정) - test구간 이후 시점 포함\n",
    "X_init = train_data[sensor_cols].values[-60:]\n",
    "X_window = X_init.copy()\n",
    "\n",
    "gru_first_y_pred = gru_trend_model.predict(X_window.reshape(1, 60, len(sensor_cols)))\n",
    "gru_y_pred_rest = gru_trend_model.predict(X_test)\n",
    "gru_y_pred_full = np.vstack([gru_first_y_pred, gru_y_pred_rest])\n",
    "\n",
    "future_steps = 10080\n",
    "\n",
    "future_preds = []\n",
    "last_input = X_test[-1:].copy()\n",
    "\n",
    "for _ in range(future_steps):\n",
    "    pred = gru_trend_model.predict(last_input)\n",
    "    future_preds.append(pred[0])\n",
    "    last_input = np.append(last_input[:, 1:, :], pred.reshape(1, 1, -1), axis=1)\n",
    "future_preds = np.array(future_preds)\n",
    "\n",
    "train_values_orig = scaler.inverse_transform(train_data[sensor_cols].values)\n",
    "y_pred_orig = scaler.inverse_transform(gru_y_pred_full)\n",
    "future_pred_orig = scaler.inverse_transform(future_preds)\n",
    "y_test_orig = scaler.inverse_transform(y_test)\n",
    "\n",
    "time_gap = test_data.index[1] - test_data.index[0]\n",
    "first_pred_time = train_data.index[-1] + time_gap\n",
    "pred_timestamps = [first_pred_time + i * time_gap for i in range(len(gru_y_pred_full))]\n",
    "\n",
    "last_pred_time = pred_timestamps[-1] + time_gap\n",
    "future_timestamps = [last_pred_time + i * time_gap for i in range(future_steps)]\n",
    "\n",
    "train_orig = train_values_orig\n",
    "pred_orig = y_pred_orig\n",
    "test_orig = y_test_orig\n",
    "future_orig = future_pred_orig\n",
    "\n",
    "for i, sensor in enumerate(sensor_cols):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(train_data.index, train_orig[:, i], label=\"Train\", color=\"black\")\n",
    "    plt.plot(pred_timestamps, pred_orig[:, i], label=\"Test Predicted\", color=\"orange\")\n",
    "    plt.plot(pred_timestamps[1:], test_orig[:, i], label=\"Test Actual\", color=\"dodgerblue\", linestyle='--')\n",
    "    plt.plot(future_timestamps, future_orig[:, i], label=\"Future Forecast (+7d)\", color=\"green\", linestyle=':')\n",
    "    plt.title(f\"Sensor Forecast - {sensor} (GRU)\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"Sensor Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58d9ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12661139496702322710\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
